{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, KFold\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\librosa\\display.py:44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m colormaps \u001b[38;5;28;01mas\u001b[39;00m mcm\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmplaxes\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmplticker\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import tensorflow as tf\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "Root_dir = 'C:/Users/Yubraj/Desktop/miky/miky/Dataset'\n",
    "Crema_path = Root_dir + \"/Crema/\" \n",
    "Ravdess_path = Root_dir + \"/Ravdess/\"\n",
    "Savee_path = Root_dir + \"/Savee/\"\n",
    "Tess_path = Root_dir + \"/Tess/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the number of samples to 300 from each dataset for increased dataset size\n",
    "Crema_dir_list = os.listdir(Crema_path)[:300]\n",
    "Ravdess_dir_list = os.listdir(Ravdess_path)[:300]\n",
    "Savee_dir_list = os.listdir(Savee_path)[:300]\n",
    "Tess_dir_list = os.listdir(Tess_path)[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess datasets\n",
    "# Crema dataset\n",
    "emotions_crema = []\n",
    "paths_crema = []\n",
    "for it in Crema_dir_list:\n",
    "    paths_crema.append(Crema_path + it)\n",
    "    part = it.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        emotions_crema.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        emotions_crema.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        emotions_crema.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        emotions_crema.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        emotions_crema.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        emotions_crema.append('neutral')\n",
    "    else:\n",
    "        emotions_crema.append('Unknown')\n",
    "\n",
    "emotions_crema_df = pd.DataFrame(emotions_crema, columns=['Emotions'])\n",
    "path_crema_df = pd.DataFrame(paths_crema, columns=['Path'])\n",
    "Crema_df = pd.concat([emotions_crema_df, path_crema_df], axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ravdess dataset\n",
    "emotions_ravdess = []\n",
    "path_ravdess = []\n",
    "for it in Ravdess_dir_list:\n",
    "    actor = os.listdir(Ravdess_path + it)[:300]\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        emotions_ravdess.append(int(part[2]))\n",
    "        path_ravdess.append(Ravdess_path + it + '/' + file)\n",
    "\n",
    "emotion_ravdess_df = pd.DataFrame(emotions_ravdess, columns=['Emotions'])\n",
    "path_ravdess_df = pd.DataFrame(path_ravdess, columns=['Path'])\n",
    "Ravdess_df = pd.concat([emotion_ravdess_df, path_ravdess_df], axis=1)\n",
    "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savee dataset\n",
    "emotions_savee = []\n",
    "path_savee = []\n",
    "for it in Savee_dir_list:\n",
    "    path_savee.append(Savee_path + it)\n",
    "    part = it.split('_')[1]\n",
    "    part = part[:-6]\n",
    "    if part == 'a':\n",
    "        emotions_savee.append('angry')\n",
    "    elif part == 'd':\n",
    "        emotions_savee.append('disgust')\n",
    "    elif part == 'f':\n",
    "        emotions_savee.append('fear')\n",
    "    elif part == 'h':\n",
    "        emotions_savee.append('happiness')\n",
    "    elif part == 'n':\n",
    "        emotions_savee.append('neutral')\n",
    "    elif part == 'sa':\n",
    "        emotions_savee.append('sadness')\n",
    "    elif part == 'su':\n",
    "        emotions_savee.append('surprise')\n",
    "    else:\n",
    "        emotions_savee.append('Unknown')\n",
    "\n",
    "emotion_savee_df = pd.DataFrame(emotions_savee, columns=['Emotions'])\n",
    "path_savee_df = pd.DataFrame(path_savee, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_savee_df, path_savee_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tess dataset\n",
    "emotions_tess = []\n",
    "path_tess = []\n",
    "for it in Tess_dir_list:\n",
    "    directories = os.listdir(Tess_path + '/' + it)[:300]\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part == 'ps':\n",
    "            emotions_tess.append('surprise')\n",
    "        else:\n",
    "            emotions_tess.append(part)\n",
    "        path_tess.append(Tess_path + it + '/' + file)\n",
    "\n",
    "emotion_tess_df = pd.DataFrame(emotions_tess, columns=['Emotions'])\n",
    "path_tess_df = pd.DataFrame(path_tess, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_tess_df, path_tess_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all datasets\n",
    "data_path = []\n",
    "data_emotion = []\n",
    "\n",
    "def append_data(dataset):\n",
    "    for path, emotion in zip(dataset.Path, dataset.Emotions):\n",
    "        data_path.append(path)\n",
    "        data_emotion.append(emotion)\n",
    "\n",
    "append_data(Crema_df)\n",
    "append_data(Ravdess_df)\n",
    "append_data(Savee_df)\n",
    "append_data(Tess_df)\n",
    "\n",
    "All_data = pd.DataFrame(data_emotion, columns=['Emotions'])\n",
    "All_data['Path'] = data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREMA Dataset Sample:\n",
      "  Emotions                                               Path\n",
      "0    angry  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "1  disgust  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "2     fear  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "3    happy  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "4  neutral  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "\n",
      "RAVDESS Dataset Sample:\n",
      "  Emotions                                               Path\n",
      "0  neutral  C:/Users/User/Desktop/new/Dataset/Ravdess/Acto...\n",
      "1  neutral  C:/Users/User/Desktop/new/Dataset/Ravdess/Acto...\n",
      "2  neutral  C:/Users/User/Desktop/new/Dataset/Ravdess/Acto...\n",
      "3  neutral  C:/Users/User/Desktop/new/Dataset/Ravdess/Acto...\n",
      "4     calm  C:/Users/User/Desktop/new/Dataset/Ravdess/Acto...\n",
      "\n",
      "SAVEE Dataset Sample:\n",
      "  Emotions                                               Path\n",
      "0    angry  C:/Users/User/Desktop/new/Dataset/Savee/DC_a01...\n",
      "1    angry  C:/Users/User/Desktop/new/Dataset/Savee/DC_a02...\n",
      "2    angry  C:/Users/User/Desktop/new/Dataset/Savee/DC_a03...\n",
      "3    angry  C:/Users/User/Desktop/new/Dataset/Savee/DC_a04...\n",
      "4    angry  C:/Users/User/Desktop/new/Dataset/Savee/DC_a05...\n",
      "\n",
      "TESS Dataset Sample:\n",
      "  Emotions                                               Path\n",
      "0    angry  C:/Users/User/Desktop/new/Dataset/Tess/OAF_ang...\n",
      "1    angry  C:/Users/User/Desktop/new/Dataset/Tess/OAF_ang...\n",
      "2    angry  C:/Users/User/Desktop/new/Dataset/Tess/OAF_ang...\n",
      "3    angry  C:/Users/User/Desktop/new/Dataset/Tess/OAF_ang...\n",
      "4    angry  C:/Users/User/Desktop/new/Dataset/Tess/OAF_ang...\n",
      "\n",
      "Merged Dataset Sample:\n",
      "  Emotions                                               Path\n",
      "0    angry  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "1  disgust  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "2     fear  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "3    happy  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n",
      "4  neutral  C:/Users/User/Desktop/new/Dataset/Crema/1001_D...\n"
     ]
    }
   ],
   "source": [
    "# Display samples of each dataset\n",
    "print(\"CREMA Dataset Sample:\")\n",
    "print(Crema_df.head())\n",
    "\n",
    "print(\"\\nRAVDESS Dataset Sample:\")\n",
    "print(Ravdess_df.head())\n",
    "\n",
    "print(\"\\nSAVEE Dataset Sample:\")\n",
    "print(Savee_df.head())\n",
    "\n",
    "print(\"\\nTESS Dataset Sample:\")\n",
    "print(Tess_df.head())\n",
    "\n",
    "# Display sample of the merged dataset\n",
    "print(\"\\nMerged Dataset Sample:\")\n",
    "print(All_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "All_data['Emotion_Label'] = label_encoder.fit_transform(All_data['Emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = All_data['Path'].values\n",
    "y = All_data['Emotion_Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "def extract_features(file_path):\n",
    "    signal, sr = librosa.load(file_path, sr=44100)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=signal).T, axis=0)\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(y=signal, sr=sr).T, axis=0)\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    rms = np.mean(librosa.feature.rms(y=signal).T, axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=signal, sr=sr).T, axis=0)\n",
    "    return np.hstack([zcr, chroma_stft, mfcc, rms, mel])\n",
    "\n",
    "def parallel_feature_extraction(file_paths):\n",
    "    return Parallel(n_jobs=-1)(delayed(extract_features)(file) for file in file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features, filename):\n",
    "    joblib.dump(features, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save features if not already saved\n",
    "features_filename_train = 'X_train_features.joblib'\n",
    "features_filename_test = 'X_test_features.joblib'\n",
    "\n",
    "if not os.path.exists(features_filename_train) or not os.path.exists(features_filename_test):\n",
    "    X_train_features = np.array(parallel_feature_extraction(X_train))\n",
    "    X_test_features = np.array(parallel_feature_extraction(X_test))\n",
    "    save_features(X_train_features, features_filename_train)\n",
    "    save_features(X_test_features, features_filename_test)\n",
    "else:\n",
    "    X_train_features = load_features(features_filename_train)\n",
    "    X_test_features = load_features(features_filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "# X_train_features = np.array(parallel_feature_extraction(X_train))\n",
    "# X_test_features = np.array(parallel_feature_extraction(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_features = scaler.fit_transform(X_train_features)\n",
    "X_test_features = scaler.transform(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features for GRU input\n",
    "X_train_features = np.expand_dims(X_train_features, axis=2)\n",
    "X_test_features = np.expand_dims(X_test_features, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "model = Sequential([\n",
    "    Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train_features.shape[1], 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    GRU(256, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    GRU(128, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Early Stopping and Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 192ms/step - accuracy: 0.1774 - loss: 5.0883 - val_accuracy: 0.1952 - val_loss: 2.6683 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 172ms/step - accuracy: 0.2392 - loss: 2.2727 - val_accuracy: 0.1818 - val_loss: 2.2068 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 255ms/step - accuracy: 0.3140 - loss: 1.9008 - val_accuracy: 0.4163 - val_loss: 1.7147 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 261ms/step - accuracy: 0.4117 - loss: 1.6078 - val_accuracy: 0.4607 - val_loss: 1.5607 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 192ms/step - accuracy: 0.4886 - loss: 1.4550 - val_accuracy: 0.4762 - val_loss: 1.4588 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 252ms/step - accuracy: 0.5632 - loss: 1.3118 - val_accuracy: 0.5950 - val_loss: 1.2946 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 259ms/step - accuracy: 0.6016 - loss: 1.2251 - val_accuracy: 0.6322 - val_loss: 1.2009 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 333ms/step - accuracy: 0.6350 - loss: 1.1263 - val_accuracy: 0.6446 - val_loss: 1.0840 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 348ms/step - accuracy: 0.6676 - loss: 1.0186 - val_accuracy: 0.6467 - val_loss: 1.0698 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 281ms/step - accuracy: 0.6730 - loss: 1.0140 - val_accuracy: 0.6808 - val_loss: 1.0080 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 315ms/step - accuracy: 0.6924 - loss: 0.9382 - val_accuracy: 0.6787 - val_loss: 1.0034 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 295ms/step - accuracy: 0.6992 - loss: 0.9317 - val_accuracy: 0.6705 - val_loss: 1.0064 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 263ms/step - accuracy: 0.6625 - loss: 1.0709 - val_accuracy: 0.6777 - val_loss: 1.0749 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 309ms/step - accuracy: 0.7055 - loss: 0.9478 - val_accuracy: 0.6839 - val_loss: 0.9740 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 325ms/step - accuracy: 0.7242 - loss: 0.8830 - val_accuracy: 0.6963 - val_loss: 0.9556 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 296ms/step - accuracy: 0.7301 - loss: 0.8599 - val_accuracy: 0.6963 - val_loss: 0.9335 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 319ms/step - accuracy: 0.7378 - loss: 0.7993 - val_accuracy: 0.6942 - val_loss: 0.9551 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 328ms/step - accuracy: 0.7212 - loss: 0.8457 - val_accuracy: 0.7014 - val_loss: 0.8595 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 333ms/step - accuracy: 0.7452 - loss: 0.7902 - val_accuracy: 0.7128 - val_loss: 0.8895 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 345ms/step - accuracy: 0.7371 - loss: 0.7950 - val_accuracy: 0.6798 - val_loss: 0.9863 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 324ms/step - accuracy: 0.7164 - loss: 0.8701 - val_accuracy: 0.7190 - val_loss: 0.9109 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 343ms/step - accuracy: 0.7577 - loss: 0.7575 - val_accuracy: 0.7273 - val_loss: 0.8247 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 392ms/step - accuracy: 0.7722 - loss: 0.6871 - val_accuracy: 0.7242 - val_loss: 0.8483 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 456ms/step - accuracy: 0.7833 - loss: 0.6788 - val_accuracy: 0.7355 - val_loss: 0.7937 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 365ms/step - accuracy: 0.8059 - loss: 0.6350 - val_accuracy: 0.7159 - val_loss: 0.8110 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 363ms/step - accuracy: 0.8044 - loss: 0.6341 - val_accuracy: 0.7273 - val_loss: 0.7873 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 386ms/step - accuracy: 0.7901 - loss: 0.6619 - val_accuracy: 0.7314 - val_loss: 0.7908 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 350ms/step - accuracy: 0.8018 - loss: 0.6216 - val_accuracy: 0.7273 - val_loss: 0.7857 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 357ms/step - accuracy: 0.8001 - loss: 0.6183 - val_accuracy: 0.7324 - val_loss: 0.7950 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 490ms/step - accuracy: 0.7935 - loss: 0.6252 - val_accuracy: 0.7314 - val_loss: 0.8046 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "history = model.fit(X_train_features, y_train, epochs=30, batch_size=32, validation_data=(X_test_features, y_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - accuracy: 0.7171 - loss: 0.8082\n",
      "Test Accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test_features, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.7171 - loss: 0.8082\n",
      "Test Loss: 0.7857\n",
      "Test Accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test_features, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model, encoder, and scaler\n",
    "model.save('final_audio_emotion_model.h5')\n",
    "dump(label_encoder, 'label_encoder.joblib')\n",
    "dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.84      0.70      0.77       138\n",
      "        calm       0.36      0.61      0.45        41\n",
      "     disgust       0.64      0.71      0.67       136\n",
      "        fear       0.75      0.77      0.76       146\n",
      "   happiness       0.29      0.40      0.33        10\n",
      "       happy       0.85      0.65      0.74       135\n",
      "     neutral       1.00      0.77      0.87       109\n",
      "         sad       0.71      0.72      0.72       122\n",
      "     sadness       0.33      1.00      0.50         3\n",
      "    surprise       0.72      0.83      0.77       128\n",
      "\n",
      "    accuracy                           0.73       968\n",
      "   macro avg       0.65      0.72      0.66       968\n",
      "weighted avg       0.76      0.73      0.74       968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test_features)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred_labels, target_names=label_encoder.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(file_path):\n",
    "    # Extract features\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    # Normalize features\n",
    "    features = scaler.transform([features])\n",
    "    \n",
    "    # Reshape features for GRU input\n",
    "    features = np.expand_dims(features, axis=2)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(features)\n",
    "    y_pred_label = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Decode label to emotion\n",
    "    predicted_emotion = label_encoder.inverse_transform(y_pred_label)\n",
    "    \n",
    "    return predicted_emotion[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "Predicted Emotion: fear\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "sample_path = \"C:/Users/User/Desktop/new/Dataset/Crema/1001_MTI_HAP_XX.wav\"\n",
    "predicted_emotion = predict_emotion(sample_path)\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
